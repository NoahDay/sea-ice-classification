{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f9a63c-51f5-4067-a7a6-755d704d2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "#from kneed import KneeLocator\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import datetime as dt\n",
    "import scipy.io\n",
    "import glob\n",
    "import pyresample\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "import netCDF4\n",
    "import datetime as dt\n",
    "from netCDF4 import date2num,num2date\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib as mpl\n",
    "from netCDF4 import Dataset\n",
    "import IPython.display\n",
    "import cmocean\n",
    "import cmocean.cm as cmo\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cft\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib import rc\n",
    "#import cosima_cookbook as cc\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def get_var_list(list_name):\n",
    "    '''\n",
    "    Get a list of variables associated with a pre-defined name.\n",
    "    '''\n",
    "    if list_name == 'ocn':\n",
    "        var_list = ['sst','sss','uocn','vocn','frzmlt']\n",
    "    elif list_name == 'atmo':\n",
    "        var_list = ['Tair','uatm','vatm','fswdn','flwdn','snow']\n",
    "    elif list_name == 'wave':\n",
    "        var_list = ['aice','wave_sig_ht','peak_period','mean_wave_dir']\n",
    "    elif list_name == 'ice':\n",
    "        var_list = ['aice','hi','fsdrad','iage','uvel','vvel','frazil','congel']\n",
    "    elif list_name == 'JRA55':\n",
    "            var_list = ['airtmp']\n",
    "    elif list_name == 'static':\n",
    "#        var_list = ['aice','hi','hs','fsdrad','sice','iage','vlvl','vrdg']\n",
    "        var_list = ['aice','hi','hs','fsdrad','iage','alvl']\n",
    "    elif list_name == 'analysis':\n",
    "        var_list = ['daidtt','daidtd','Tsfc','shear','divu','strength','frazil','congel','Tair','trsig','uvel','vvel','strairx','strairy','strocnx','strocny','strintx','strinty','strcorx','strcory','wave_sig_ht','peak_period','sst','frzmlt']\n",
    "    else:\n",
    "        var_list = [list_name]\n",
    "\n",
    "    return var_list\n",
    "\n",
    "def ProgressBar(Total, Progress, BarLength=20, ProgressIcon=\"#\", BarIcon=\"-\"):\n",
    "    try:\n",
    "        # You can't have a progress bar with zero or negative length.\n",
    "        if BarLength <1:\n",
    "            BarLength = 20\n",
    "        # Use status variable for going to the next line after progress completion.\n",
    "        Status = \"\"\n",
    "        # Calcuting progress between 0 and 1 for percentage.\n",
    "        Progress = float(Progress) / float(Total)\n",
    "        # Doing this conditions at final progressing.\n",
    "        if Progress >= 1.:\n",
    "            Progress = 1\n",
    "            Status = \"\\r\\n\"    # Going to the next line\n",
    "        # Calculating how many places should be filled\n",
    "        Block = int(round(BarLength * Progress))\n",
    "        # Show this\n",
    "        Bar = \"[{}] {:.0f}% {}\".format(ProgressIcon * Block + BarIcon * (BarLength - Block), round(Progress * 100, 0), Status)\n",
    "        return Bar\n",
    "    except:\n",
    "        return \"ERROR\"\n",
    "\n",
    "\n",
    "def ShowBar(Bar):\n",
    "    sys.stdout.write(Bar)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def cice_netcdf_to_df(mypath, year):\n",
    "    '''\n",
    "    Convert a year of CICE history files into a pandas dataframe.\n",
    "    '''\n",
    "    os.chdir(mypath)\n",
    "    file_dates = []\n",
    "    print(year)\n",
    "    filename =  mypath + 'iceh.' + str(year) + '-01-01.nc'\n",
    "\n",
    "    onlyfiles = glob.glob(\"{path}/iceh.*{year}*\".format(path=mypath, year=year))\n",
    "    onlyfiles.sort()\n",
    "\n",
    "    ds = xr.open_dataset(filename)\n",
    "    LN = ds.TLON.values\n",
    "    LT = ds.TLAT.values\n",
    "    # Get the total number of grid points\n",
    "    size = 1\n",
    "    for dim in np.shape(LN): size *= dim\n",
    "    aice_data = ds['aice'][0,:,:]\n",
    "    mask1 = np.ma.masked_where(LT > 0.0, aice_data)\n",
    "    mask2 = np.ma.masked_where(aice_data < 0.15, aice_data)\n",
    "    master_mask = mask1.mask | mask2.mask\n",
    "    mask = master_mask\n",
    "\n",
    "    X_out =  np.ma.masked_array(np.empty((size,1)), mask=mask)\n",
    "    \n",
    "    # Loop over the files in that year\n",
    "    for filecount, file in enumerate(onlyfiles):\n",
    "        progressBar = \"\\rProgress: \" + ProgressBar(len(onlyfiles), filecount+1, 20, '#', '.')\n",
    "        ShowBar(progressBar)\n",
    "        \n",
    "        # Open the file\n",
    "        filename = file\n",
    "        file_dates.append(np.datetime64(file[-13:-3]))\n",
    "        ds = xr.open_dataset(filename)\n",
    "        \n",
    "        # Get and apply masks to remove the ocean\n",
    "        aice_data = ds['aice'][0,:,:]\n",
    "        mask1 = np.ma.masked_where(LT > 0.0, aice_data)\n",
    "        mask2 = np.ma.masked_where(aice_data < 0.15, aice_data)\n",
    "        master_mask = mask1.mask | mask2.mask\n",
    "        mask = master_mask\n",
    "        \n",
    "        # Get all the variables\n",
    "        for counter, exp in enumerate(variable_list):\n",
    "            data = ds[exp][0,:,:]\n",
    "            data_masked = np.ma.masked_where(mask, data.values)\n",
    "            data_masked_vec = data_masked.compressed()\n",
    "            row_length, = data_masked_vec.shape\n",
    "\n",
    "            if counter == 0: \n",
    "                # First file, then initialise X_temp\n",
    "                X_single_file = data_masked_vec.reshape(row_length,1)\n",
    "            else:\n",
    "                # Else just concatenate the new data on\n",
    "                X_single_file = np.concatenate([X_single_file, data_masked_vec.reshape(row_length,1)],axis=1)\n",
    "\n",
    "        # Add on the corresponding coordinates\n",
    "        LN_masked = np.ma.masked_where(mask, LN)\n",
    "        LN_vec = LN_masked.compressed()\n",
    "        LT_masked = np.ma.masked_where(mask, LT)\n",
    "        LT_vec = LT_masked.compressed()\n",
    "        X_single_file = np.concatenate([X_single_file, LN_vec.reshape(row_length,1), LT_vec.reshape(row_length,1)],axis=1)\n",
    "        \n",
    "        if filecount == 0: \n",
    "            # Day 1, then initialise the year file\n",
    "            X_year = X_single_file\n",
    "            datetime_vec =  np.tile(np.datetime64(file[-13:-3]),(row_length,1))\n",
    "        else:\n",
    "            X_year = np.concatenate([X_year, X_single_file],axis=0)\n",
    "            datetime_vec = np.concatenate([datetime_vec, np.tile(np.datetime64(file[-13:-3]),(row_length,1))],axis=0)\n",
    "    # Save as dataframe\n",
    "    df_raw = pd.DataFrame(X_year, columns = variable_list+['longitude','latitude'])#,'date'])\n",
    "    df_raw['date'] = datetime_vec\n",
    "    print(datetime_vec.shape)\n",
    "    df_raw = df_raw.dropna()\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "def standardise(df_raw):\n",
    "    X_temp = df_raw['aice'].values\n",
    "    len_X, = X_temp.shape\n",
    "    row_index = len_X\n",
    "    X_train = np.zeros((row_index,1))\n",
    "    for counter, exp in enumerate(variable_list):\n",
    "#        progressBar = \"\\rProgress: \" + ProgressBar(counter, len(variable_list), 20, '#', '.')\n",
    "#        ShowBar(progressBar)\n",
    "        \n",
    "        X_temp_vec = df_raw[exp].values\n",
    "\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        X_temp_vec = min_max_scaler.fit_transform(X_temp_vec.reshape(row_index,1))\n",
    "\n",
    "        # Log transformation\n",
    "       # X_temp_vec = np.log(X_temp_vec+1)\n",
    "       # scaler = StandardScaler()\n",
    "#        X_temp_vec = min_max_scaler.fit_transform(X_temp_vec.reshape(row_index,1))\n",
    "        X_train = np.concatenate([X_train, X_temp_vec],axis=1)\n",
    "\n",
    "    temp_lon = df_raw['longitude'].to_numpy()\n",
    "    temp_lat = df_raw['latitude'].to_numpy()\n",
    "    X_train = np.concatenate([X_train, temp_lon.reshape(row_index,1), temp_lat.reshape(row_index,1)],axis=1)\n",
    "    X_train_out = np.delete(X_train,0,1)\n",
    "\n",
    "    df_standard = pd.DataFrame(X_train_out, columns = variable_list+['longitude','latitude'])\n",
    "    df_standard['date'] = df_raw['date'].values\n",
    "    #print(df_standard.describe())\n",
    "    return df_standard\n",
    "\n",
    "def kmeans_cluster(df_raw, df_standard):\n",
    "    init_centroids_good = np.array([[0.76306007, 0.04041951, 0.02538844, 0.02783396, 0.05290037, 0.74416677],\n",
    "                                    [0.96479608, 0.11019326, 0.10738233, 0.26478933, 0.07206991, 0.80610988],\n",
    "                                    [0.96413898, 0.18858583, 0.14985236, 0.82837526, 0.10536767, 0.74644532]])\n",
    "    kmeans = KMeans(\n",
    "        init=init_centroids_good,\n",
    "        n_clusters=3,\n",
    "        n_init=1,\n",
    "        max_iter=1,\n",
    "        random_state=2020\n",
    "    )\n",
    "    \n",
    "    # Take a sub-sample (same number of points for each date)\n",
    "    df_temp = df_standard.drop(df_raw[df_raw.aice < 0.01].index)\n",
    "    df_subsample = df_temp.groupby('date', group_keys=False).apply(lambda x: x.sample(500))\n",
    "    X_train = df_subsample.iloc[:, 1:7] \n",
    "\n",
    "    kmeans.fit(X_train)\n",
    "    kmeans.cluster_centers_ = init_centroids_good\n",
    "\n",
    "    X_all = df_standard.iloc[:, 1:7] \n",
    "    predicted = kmeans.predict(X_all) \n",
    "\n",
    "    df_kmeans = df_standard\n",
    "    df_kmeans['k'] = predicted\n",
    "    \n",
    "    print(kmeans.cluster_centers_)\n",
    "    \n",
    "    return df_kmeans\n",
    "\n",
    "def map_to_netcdf(df_kmeans,savefilename,mypath,year):\n",
    "    os.chdir(mypath)\n",
    "    filename =  mypath + 'iceh.' + str(year) + '-01-01.nc'\n",
    "    onlyfiles = glob.glob(\"{path}/iceh.*{year}*\".format(path=mypath, year=year))\n",
    "    onlyfiles.sort()\n",
    "    \n",
    "    filename = '/g/data/ia40/cice-dirs/runs/waves-10/history/iceh.2000-01-01.nc'\n",
    "    ds = xr.open_dataset(filename)\n",
    "    LN = ds.TLON.values\n",
    "    LT = ds.TLAT.values\n",
    "    \n",
    "    unique_dates = df_kmeans['date'].unique()\n",
    "    tmp1, = unique_dates.shape\n",
    "\n",
    "    tmp2, tmp3 = LN.shape\n",
    "    k_means_array = np.empty((tmp1,tmp2,tmp3))\n",
    "    k_means_array[:] = np.nan\n",
    "\n",
    "    date_length, = unique_dates.shape\n",
    "    # Map the k values onto the grid\n",
    "    for time_lp, date_tmp in enumerate(unique_dates):\n",
    "        progressBar = \"\\rProgress: \" + ProgressBar(date_length, time_lp+1, 20, '#', '.')\n",
    "        ShowBar(progressBar)\n",
    "        date_idx = df_kmeans['date'] == date_tmp\n",
    "\n",
    "        lon = df_kmeans['longitude'][date_idx];\n",
    "        lat = df_kmeans['latitude'][date_idx];\n",
    "        k = df_kmeans['k'][date_idx];\n",
    "\n",
    "        row_length = date_idx.sum()\n",
    "        lon = lon.values.reshape(row_length,1)\n",
    "        lat = lat.values.reshape(row_length,1)\n",
    "        k = k.values.reshape(row_length,1)\n",
    "        for row_lp in range(0, row_length):\n",
    "            a = abs(LT-lat[row_lp])+abs(LN-lon[row_lp])\n",
    "            i,j = np.unravel_index(a.argmin(),a.shape)\n",
    "            k_means_array[time_lp,i,j] = k[row_lp]\n",
    "  \n",
    "    # Training variables for k-means\n",
    "    aice_array = np.empty((tmp1,tmp2,tmp3))\n",
    "    hi_array = np.empty((tmp1,tmp2,tmp3))\n",
    "    hs_array = np.empty((tmp1,tmp2,tmp3))\n",
    "    fsdrad_array = np.empty((tmp1,tmp2,tmp3))\n",
    "    iage_array = np.empty((tmp1,tmp2,tmp3))\n",
    "    alvl_array = np.empty((tmp1,tmp2,tmp3))\n",
    "\n",
    "    # Extra variables of interest\n",
    "    swh_array = np.empty((tmp1,tmp2,tmp3))\n",
    "    ppd_array = np.empty((tmp1,tmp2,tmp3))\n",
    "\n",
    "\n",
    "    date_length, = unique_dates.shape\n",
    "\n",
    "    for filecount, file in enumerate(onlyfiles):\n",
    "        filename = file\n",
    "        progressBar = \"\\rProgress: \" + ProgressBar(date_length, filecount+1, 20, '#', '.')\n",
    "        ShowBar(progressBar)\n",
    "\n",
    "        ds = xr.open_dataset(filename)\n",
    "        LN = ds.TLON.values\n",
    "        LT = ds.TLAT.values\n",
    "\n",
    "        aice_data = ds['aice'][0,:,:]\n",
    "\n",
    "        mask1 = np.ma.masked_where(LT > 0.0, aice_data)\n",
    "        mask2 = np.ma.masked_where(aice_data < 0.15, aice_data)\n",
    "        master_mask = mask1.mask | mask2.mask\n",
    "        mask = master_mask\n",
    "\n",
    "        data = ds['aice'][0,:,:]\n",
    "        aice_array[filecount,:,:] = data\n",
    "        aice_array[filecount,mask] = np.nan\n",
    "\n",
    "        data = ds['hi'][0,:,:]\n",
    "        hi_array[filecount,:,:] = data\n",
    "        hi_array[filecount,mask] = np.nan\n",
    "\n",
    "        data = ds['hs'][0,:,:]\n",
    "        hs_array[filecount,:,:] = data\n",
    "        hs_array[filecount,mask] = np.nan\n",
    "\n",
    "        data = ds['fsdrad'][0,:,:]\n",
    "        fsdrad_array[filecount,:,:] = data\n",
    "        fsdrad_array[filecount,mask] = np.nan\n",
    "\n",
    "        data = ds['iage'][0,:,:]\n",
    "        iage_array[filecount,:,:] = data\n",
    "        iage_array[filecount,mask] = np.nan\n",
    "\n",
    "        data = ds['alvl'][0,:,:]\n",
    "        alvl_array[filecount,:,:] = data\n",
    "        alvl_array[filecount,mask] = np.nan\n",
    "\n",
    "        # Extra variables of interest\n",
    "        data = ds['wave_sig_ht'][0,:,:]\n",
    "        swh_array[filecount,:,:] = data\n",
    "        swh_array[filecount,mask] = np.nan\n",
    "\n",
    "        data = ds['peak_period'][0,:,:]\n",
    "        ppd_array[filecount,:,:] = data\n",
    "        ppd_array[filecount,mask] = np.nan\n",
    "    \n",
    "    # Save to netCDF\n",
    "    ds = xr.open_dataset(filename)\n",
    "    HTE = ds.HTE.values\n",
    "    HTN = ds.HTN.values\n",
    "    tarea = ds.tarea.values\n",
    "    tmask = ds.tmask.values\n",
    "    \n",
    "    d_vars = {\"k\" : (['time','nj','ni'],k_means_array,\n",
    "                                  {'long_name' :\"k-means_clusters\",\n",
    "                                   'units'     :\"cluster number\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"aice\" : (['time','nj','ni'],aice_array,\n",
    "                                  {'long_name' :\"Areal sea ice area proportion of cell\",\n",
    "                                   'units'     :\"-\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"hi\" : (['time','nj','ni'],hi_array,\n",
    "                                  {'long_name' :\"Grid cell mean ice thickness\",\n",
    "                                   'units'     :\"m\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"hs\" : (['time','nj','ni'],hs_array,\n",
    "                                  {'long_name' :\"Grid cell mean snow thickness\",\n",
    "                                   'units'     :\"m\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"fsdrad\" : (['time','nj','ni'],fsdrad_array,\n",
    "                                  {'long_name' :\"Representative floe radius\",\n",
    "                                   'units'     :\"m\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"iage\" : (['time','nj','ni'],iage_array,\n",
    "                                  {'long_name' :\"Sea ice age\",\n",
    "                                   'units'     :\"m\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"alvl\" : (['time','nj','ni'],alvl_array,\n",
    "                                  {'long_name' :\"Level ice area fraction\",\n",
    "                                   'units'     :\"-\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"wave_sig_ht\" : (['time','nj','ni'],swh_array,\n",
    "                                  {'long_name' :\"Signficant wave height\",\n",
    "                                   'units'     :\"m\",\n",
    "                                   '_FillValue':-2e8}),\n",
    "              \"HTE\" : (['nj','ni'],HTE,\n",
    "                              {'long_name':\"T cell width on East side\",\n",
    "                               'units'    :\"m\",\n",
    "                               '_FillValue':-2e8}),\n",
    "              \"HTN\" : (['nj','ni'],HTN,\n",
    "                              {'long_name':\"T cell width on North side\",\n",
    "                               'units'    :\"m\",\n",
    "                               '_FillValue':-2e8}),\n",
    "              \"tarea\" : (['nj','ni'],tarea,\n",
    "                              {'long_name':\"area of T grid cells\",\n",
    "                               'units'    :\"m^2\",\n",
    "                               '_FillValue':-2e8}),\n",
    "              \"tmask\" : (['nj','ni'],tmask,\n",
    "                              {'long_name':\"ocean grid mask\",\n",
    "                               'units'    :\"Boolean\",\n",
    "                               '_FillValue':-2e8})}\n",
    "\n",
    "    coords = {\"LON\"  : ([\"nj\",\"ni\"],LN,{'units':'degrees_east'}),\n",
    "              \"LAT\"  : ([\"nj\",\"ni\"],LT,{'units':'degrees_north'}),\n",
    "              \"time\" : ([\"time\"],unique_dates)}\n",
    "    attrs = {'creation_date': \"2023-04-07\",#datetime.now().strftime('%Y-%m-%d %H'),\n",
    "             'conventions'  : \"\",\n",
    "             'title'        : \"k-means clusters for CICE-WIM standalone 1-degree data\",\n",
    "             'source'       : \", \",\n",
    "             'comment'      : \"\",\n",
    "             'author'       : 'Noah Day',\n",
    "             'email'        : 'noah.day@adelaide.edu.au'}\n",
    "    enc_dict  = {'shuffle':True,'zlib':True,'complevel':5} \n",
    "    nc_out = xr.Dataset(data_vars=d_vars,coords=coords,attrs=attrs)\n",
    "    write_job = nc_out.to_netcdf(savefilename,unlimited_dims=['time'],compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3850a25d-063e-455d-a8ab-f7a53045d9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "Progress: [####################] 100% \n",
      "(6809572, 1)\n"
     ]
    }
   ],
   "source": [
    "# Read in the CICE data\n",
    "var_name = 'static'\n",
    "variable_list = get_var_list(var_name)\n",
    "num_variables = np.size(variable_list)\n",
    "\n",
    "savepath = '/home/566/nd0349/notebooks/'\n",
    "mypath = '/g/data/ia40/cice-dirs/runs/waves-10/history/'\n",
    "year = 2011\n",
    "\n",
    "df = cice_netcdf_to_df(mypath, year)\n",
    "\n",
    "savepath = '/g/data/ia40/sea-ice-classification/dataframes/'\n",
    "savefilename = 'raw_'+str(year)+'.csv'\n",
    "df.to_csv(savepath+savefilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc5bdd7b-787f-4773-9814-bef1b5509584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aice</th>\n",
       "      <th>hi</th>\n",
       "      <th>hs</th>\n",
       "      <th>fsdrad</th>\n",
       "      <th>iage</th>\n",
       "      <th>alvl</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.137572e+06</td>\n",
       "      <td>2.137572e+06</td>\n",
       "      <td>2.137572e+06</td>\n",
       "      <td>2.137572e+06</td>\n",
       "      <td>2.137572e+06</td>\n",
       "      <td>2.137572e+06</td>\n",
       "      <td>2.137572e+06</td>\n",
       "      <td>2.137572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.307068e-01</td>\n",
       "      <td>1.673727e-01</td>\n",
       "      <td>1.393969e-01</td>\n",
       "      <td>6.330059e-01</td>\n",
       "      <td>8.348495e-02</td>\n",
       "      <td>7.545040e-01</td>\n",
       "      <td>2.064478e+02</td>\n",
       "      <td>-6.846028e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.699783e-01</td>\n",
       "      <td>1.035059e-01</td>\n",
       "      <td>1.177725e-01</td>\n",
       "      <td>3.405173e-01</td>\n",
       "      <td>6.027172e-02</td>\n",
       "      <td>2.133889e-01</td>\n",
       "      <td>1.000018e+02</td>\n",
       "      <td>4.996740e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-7.762990e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.625749e-01</td>\n",
       "      <td>8.621833e-02</td>\n",
       "      <td>4.784778e-02</td>\n",
       "      <td>3.664212e-01</td>\n",
       "      <td>3.675344e-02</td>\n",
       "      <td>6.504778e-01</td>\n",
       "      <td>1.525000e+02</td>\n",
       "      <td>-7.243740e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.969490e-01</td>\n",
       "      <td>1.553418e-01</td>\n",
       "      <td>1.098350e-01</td>\n",
       "      <td>7.395617e-01</td>\n",
       "      <td>7.140754e-02</td>\n",
       "      <td>8.222344e-01</td>\n",
       "      <td>2.095000e+02</td>\n",
       "      <td>-6.870660e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.994976e-01</td>\n",
       "      <td>2.357784e-01</td>\n",
       "      <td>2.022592e-01</td>\n",
       "      <td>9.434922e-01</td>\n",
       "      <td>1.184501e-01</td>\n",
       "      <td>9.174587e-01</td>\n",
       "      <td>3.045000e+02</td>\n",
       "      <td>-6.503162e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.595000e+02</td>\n",
       "      <td>-5.303830e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               aice            hi            hs        fsdrad          iage  \\\n",
       "count  2.137572e+06  2.137572e+06  2.137572e+06  2.137572e+06  2.137572e+06   \n",
       "mean   9.307068e-01  1.673727e-01  1.393969e-01  6.330059e-01  8.348495e-02   \n",
       "std    1.699783e-01  1.035059e-01  1.177725e-01  3.405173e-01  6.027172e-02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    9.625749e-01  8.621833e-02  4.784778e-02  3.664212e-01  3.675344e-02   \n",
       "50%    9.969490e-01  1.553418e-01  1.098350e-01  7.395617e-01  7.140754e-02   \n",
       "75%    9.994976e-01  2.357784e-01  2.022592e-01  9.434922e-01  1.184501e-01   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "               alvl     longitude      latitude  \n",
       "count  2.137572e+06  2.137572e+06  2.137572e+06  \n",
       "mean   7.545040e-01  2.064478e+02 -6.846028e+01  \n",
       "std    2.133889e-01  1.000018e+02  4.996740e+00  \n",
       "min    0.000000e+00  5.000000e-01 -7.762990e+01  \n",
       "25%    6.504778e-01  1.525000e+02 -7.243740e+01  \n",
       "50%    8.222344e-01  2.095000e+02 -6.870660e+01  \n",
       "75%    9.174587e-01  3.045000e+02 -6.503162e+01  \n",
       "max    1.000000e+00  3.595000e+02 -5.303830e+01  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardise the variables\n",
    "var_name = 'static'\n",
    "variable_list = get_var_list(var_name)\n",
    "num_variables = np.size(variable_list)\n",
    "mypath = '/g/data/ia40/cice-dirs/runs/waves-10/history/'\n",
    "savepath = '/home/566/nd0349/notebooks/'\n",
    "\n",
    "df_raw = pd.read_csv('/g/data/ia40/sea-ice-classification/dataframes/raw_'+str(year)+'.csv')\n",
    "df_standard = standardise(df_raw)\n",
    "savepath = '/g/data/ia40/sea-ice-classification/dataframes/'\n",
    "savefilename = 'standard_'+str(year)+'.csv'\n",
    "df_standard.to_csv(savepath+savefilename)\n",
    "\n",
    "df_standard.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2817c69-46a2-47af-8ba3-97f91723a411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76306007 0.04041951 0.02538844 0.02783396 0.05290037 0.74416677]\n",
      " [0.96479608 0.11019326 0.10738233 0.26478933 0.07206991 0.80610988]\n",
      " [0.96413898 0.18858583 0.14985236 0.82837526 0.10536767 0.74644532]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">Unnamed: 0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">aice</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">longitude</th>\n",
       "      <th colspan=\"8\" halign=\"left\">latitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260229.0</td>\n",
       "      <td>1.036736e+06</td>\n",
       "      <td>662146.865965</td>\n",
       "      <td>675.0</td>\n",
       "      <td>468618.00</td>\n",
       "      <td>952117.0</td>\n",
       "      <td>1630154.00</td>\n",
       "      <td>2155279.0</td>\n",
       "      <td>260229.0</td>\n",
       "      <td>0.723517</td>\n",
       "      <td>...</td>\n",
       "      <td>265.5</td>\n",
       "      <td>359.5</td>\n",
       "      <td>260229.0</td>\n",
       "      <td>-62.902863</td>\n",
       "      <td>4.094776</td>\n",
       "      <td>-77.6299</td>\n",
       "      <td>-65.03162</td>\n",
       "      <td>-62.599540</td>\n",
       "      <td>-59.922382</td>\n",
       "      <td>-53.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>555132.0</td>\n",
       "      <td>9.423372e+05</td>\n",
       "      <td>505026.692344</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>514295.75</td>\n",
       "      <td>899658.5</td>\n",
       "      <td>1345712.25</td>\n",
       "      <td>2155134.0</td>\n",
       "      <td>555132.0</td>\n",
       "      <td>0.983614</td>\n",
       "      <td>...</td>\n",
       "      <td>253.5</td>\n",
       "      <td>359.5</td>\n",
       "      <td>555132.0</td>\n",
       "      <td>-64.749081</td>\n",
       "      <td>3.688464</td>\n",
       "      <td>-77.6299</td>\n",
       "      <td>-66.69558</td>\n",
       "      <td>-64.446594</td>\n",
       "      <td>-62.599540</td>\n",
       "      <td>-53.858738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1339919.0</td>\n",
       "      <td>1.141639e+06</td>\n",
       "      <td>647948.250059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>578220.50</td>\n",
       "      <td>1203130.0</td>\n",
       "      <td>1716856.50</td>\n",
       "      <td>2155082.0</td>\n",
       "      <td>1339919.0</td>\n",
       "      <td>0.958508</td>\n",
       "      <td>...</td>\n",
       "      <td>315.5</td>\n",
       "      <td>359.5</td>\n",
       "      <td>1339919.0</td>\n",
       "      <td>-70.900176</td>\n",
       "      <td>3.878079</td>\n",
       "      <td>-77.6299</td>\n",
       "      <td>-74.10892</td>\n",
       "      <td>-70.905850</td>\n",
       "      <td>-68.225560</td>\n",
       "      <td>-54.665924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                                             \\\n",
       "       count          mean            std     min        25%        50%   \n",
       "k                                                                         \n",
       "0   260229.0  1.036736e+06  662146.865965   675.0  468618.00   952117.0   \n",
       "1   555132.0  9.423372e+05  505026.692344  1388.0  514295.75   899658.5   \n",
       "2  1339919.0  1.141639e+06  647948.250059     0.0  578220.50  1203130.0   \n",
       "\n",
       "                               aice            ... longitude         \\\n",
       "          75%        max      count      mean  ...       75%    max   \n",
       "k                                              ...                    \n",
       "0  1630154.00  2155279.0   260229.0  0.723517  ...     265.5  359.5   \n",
       "1  1345712.25  2155134.0   555132.0  0.983614  ...     253.5  359.5   \n",
       "2  1716856.50  2155082.0  1339919.0  0.958508  ...     315.5  359.5   \n",
       "\n",
       "    latitude                                                                \\\n",
       "       count       mean       std      min       25%        50%        75%   \n",
       "k                                                                            \n",
       "0   260229.0 -62.902863  4.094776 -77.6299 -65.03162 -62.599540 -59.922382   \n",
       "1   555132.0 -64.749081  3.688464 -77.6299 -66.69558 -64.446594 -62.599540   \n",
       "2  1339919.0 -70.900176  3.878079 -77.6299 -74.10892 -70.905850 -68.225560   \n",
       "\n",
       "              \n",
       "         max  \n",
       "k             \n",
       "0 -53.038300  \n",
       "1 -53.858738  \n",
       "2 -54.665924  \n",
       "\n",
       "[3 rows x 72 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster\n",
    "df_raw = pd.read_csv('/g/data/ia40/sea-ice-classification/dataframes/raw_'+str(year)+'.csv')\n",
    "df_standard = pd.read_csv('/g/data/ia40/sea-ice-classification/dataframes/standard_'+str(year)+'.csv')\n",
    "df_kmeans = kmeans_cluster(df_raw, df_standard)\n",
    "savepath = '/g/data/ia40/sea-ice-classification/dataframes/'\n",
    "savefilename = 'kmeans_'+str(year)+'.csv'\n",
    "df_kmeans.to_csv(savepath+savefilename)\n",
    "\n",
    "\n",
    "df_kmeans.groupby('k').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab069190-2f63-4fd3-9168-5c7c6bcbf5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100% \n",
      "Progress: [####################] 100% \n"
     ]
    }
   ],
   "source": [
    "# Map to a netCDF\n",
    "savefilename='/g/data/ia40/sea-ice-classification/kmean_'+str(year)+'.nc'\n",
    "mypath = '/g/data/ia40/cice-dirs/runs/waves-10/history/'\n",
    "map_to_netcdf(df_kmeans,savefilename,mypath,year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978233a-7f24-40d4-acbb-1e4fd6259795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17473e40-8431-40f6-b4cd-2e72dfd30b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime.now().strftime('%Y-%m-%d %H')\n",
    "savefilename='/g/data/ia40/sea-ice-classification/kmean_'+str(year)+'.nc'\n",
    "savefilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74aa52b1-f1d4-4f51-bbcd-49fd0c969f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "Progress: [####################] 100% \n",
      "(6814861, 1)\n"
     ]
    }
   ],
   "source": [
    "# Analysis variables\n",
    "\n",
    "var_name = 'analysis'\n",
    "variable_list = get_var_list(var_name)\n",
    "num_variables = np.size(variable_list)\n",
    "\n",
    "savepath = '/home/566/nd0349/notebooks/'\n",
    "mypath = '/g/data/ia40/cice-dirs/runs/waves-10/history/'\n",
    "year = 2018\n",
    "\n",
    "df = cice_netcdf_to_df(mypath, year)\n",
    "savepath = '/g/data/ia40/sea-ice-classification/dataframes/'\n",
    "savefilename = 'analysis_raw_'+str(year)+'.csv'\n",
    "df.to_csv(savepath+savefilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c80cb6-2c82-40c6-9439-f519b23e54ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daidtt</th>\n",
       "      <th>daidtd</th>\n",
       "      <th>Tsfc</th>\n",
       "      <th>shear</th>\n",
       "      <th>divu</th>\n",
       "      <th>strength</th>\n",
       "      <th>frazil</th>\n",
       "      <th>congel</th>\n",
       "      <th>Tair</th>\n",
       "      <th>trsig</th>\n",
       "      <th>...</th>\n",
       "      <th>strintx</th>\n",
       "      <th>strinty</th>\n",
       "      <th>strcorx</th>\n",
       "      <th>strcory</th>\n",
       "      <th>wave_sig_ht</th>\n",
       "      <th>peak_period</th>\n",
       "      <th>sst</th>\n",
       "      <th>frzmlt</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "      <td>2.142861e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.386370e+00</td>\n",
       "      <td>-1.361813e+00</td>\n",
       "      <td>-1.209987e+01</td>\n",
       "      <td>6.736480e+00</td>\n",
       "      <td>2.993467e-01</td>\n",
       "      <td>1.076691e+04</td>\n",
       "      <td>2.352717e-01</td>\n",
       "      <td>4.932120e-01</td>\n",
       "      <td>-1.093232e+01</td>\n",
       "      <td>-8.488240e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.743997e-03</td>\n",
       "      <td>2.104725e-03</td>\n",
       "      <td>-2.631413e-03</td>\n",
       "      <td>9.051661e-05</td>\n",
       "      <td>9.599044e-02</td>\n",
       "      <td>1.206189e+01</td>\n",
       "      <td>-1.890005e+00</td>\n",
       "      <td>-2.592636e+01</td>\n",
       "      <td>2.068146e+02</td>\n",
       "      <td>-6.849773e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.114988e+00</td>\n",
       "      <td>4.530211e+00</td>\n",
       "      <td>8.954357e+00</td>\n",
       "      <td>9.532035e+00</td>\n",
       "      <td>5.511613e+00</td>\n",
       "      <td>9.596997e+03</td>\n",
       "      <td>4.552266e-01</td>\n",
       "      <td>6.343287e-01</td>\n",
       "      <td>8.595249e+00</td>\n",
       "      <td>1.015034e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.530699e-02</td>\n",
       "      <td>2.216662e-02</td>\n",
       "      <td>1.404996e-02</td>\n",
       "      <td>1.792450e-02</td>\n",
       "      <td>3.869908e-01</td>\n",
       "      <td>6.324199e+00</td>\n",
       "      <td>2.250972e-01</td>\n",
       "      <td>1.169549e+02</td>\n",
       "      <td>9.924809e+01</td>\n",
       "      <td>5.010849e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.676929e+01</td>\n",
       "      <td>-1.241699e+02</td>\n",
       "      <td>-4.357811e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.305874e+02</td>\n",
       "      <td>2.019805e-05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.681859e+01</td>\n",
       "      <td>-4.393299e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.714528e-01</td>\n",
       "      <td>-8.035051e-01</td>\n",
       "      <td>-1.460927e-01</td>\n",
       "      <td>-2.079815e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.968267e+00</td>\n",
       "      <td>-1.000000e+03</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-7.762990e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.333240e-02</td>\n",
       "      <td>-1.443266e+00</td>\n",
       "      <td>-1.936956e+01</td>\n",
       "      <td>1.826602e+00</td>\n",
       "      <td>-4.755968e-01</td>\n",
       "      <td>3.588815e+03</td>\n",
       "      <td>1.932373e-02</td>\n",
       "      <td>1.497240e-02</td>\n",
       "      <td>-1.757928e+01</td>\n",
       "      <td>-1.192001e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.014659e-03</td>\n",
       "      <td>-3.748889e-03</td>\n",
       "      <td>-8.397230e-03</td>\n",
       "      <td>-6.340887e-03</td>\n",
       "      <td>3.735831e-08</td>\n",
       "      <td>1.085520e+01</td>\n",
       "      <td>-1.926586e+00</td>\n",
       "      <td>-5.877564e+00</td>\n",
       "      <td>1.555000e+02</td>\n",
       "      <td>-7.243740e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.347195e-01</td>\n",
       "      <td>-6.565638e-01</td>\n",
       "      <td>-1.075332e+01</td>\n",
       "      <td>4.145495e+00</td>\n",
       "      <td>2.601054e-01</td>\n",
       "      <td>9.620862e+03</td>\n",
       "      <td>1.003949e-01</td>\n",
       "      <td>2.935034e-01</td>\n",
       "      <td>-9.761654e+00</td>\n",
       "      <td>-5.596223e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.316632e-06</td>\n",
       "      <td>1.612066e-03</td>\n",
       "      <td>-1.699846e-03</td>\n",
       "      <td>8.445031e-04</td>\n",
       "      <td>9.152009e-05</td>\n",
       "      <td>1.478338e+01</td>\n",
       "      <td>-1.911775e+00</td>\n",
       "      <td>3.574283e-01</td>\n",
       "      <td>2.105000e+02</td>\n",
       "      <td>-6.870660e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.353378e+00</td>\n",
       "      <td>-2.225964e-01</td>\n",
       "      <td>-3.809207e+00</td>\n",
       "      <td>7.736388e+00</td>\n",
       "      <td>1.247540e+00</td>\n",
       "      <td>1.556972e+04</td>\n",
       "      <td>2.692455e-01</td>\n",
       "      <td>7.037333e-01</td>\n",
       "      <td>-2.772018e+00</td>\n",
       "      <td>-1.606109e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.168001e-03</td>\n",
       "      <td>1.000495e-02</td>\n",
       "      <td>2.763798e-03</td>\n",
       "      <td>8.587679e-03</td>\n",
       "      <td>1.047749e-02</td>\n",
       "      <td>1.479290e+01</td>\n",
       "      <td>-1.897585e+00</td>\n",
       "      <td>2.217344e+00</td>\n",
       "      <td>3.035000e+02</td>\n",
       "      <td>-6.503162e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.937742e+02</td>\n",
       "      <td>1.046825e+02</td>\n",
       "      <td>6.099917e-07</td>\n",
       "      <td>1.939003e+02</td>\n",
       "      <td>1.768665e+02</td>\n",
       "      <td>3.218868e+05</td>\n",
       "      <td>1.397282e+01</td>\n",
       "      <td>7.270258e+00</td>\n",
       "      <td>1.498381e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001443e+00</td>\n",
       "      <td>8.877492e-01</td>\n",
       "      <td>1.287962e-01</td>\n",
       "      <td>1.221347e-01</td>\n",
       "      <td>9.245866e+00</td>\n",
       "      <td>9.999902e+02</td>\n",
       "      <td>4.755164e+00</td>\n",
       "      <td>1.381290e+02</td>\n",
       "      <td>3.595000e+02</td>\n",
       "      <td>-5.303830e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             daidtt        daidtd          Tsfc         shear          divu  \\\n",
       "count  2.142861e+06  2.142861e+06  2.142861e+06  2.142861e+06  2.142861e+06   \n",
       "mean   1.386370e+00 -1.361813e+00 -1.209987e+01  6.736480e+00  2.993467e-01   \n",
       "std    7.114988e+00  4.530211e+00  8.954357e+00  9.532035e+00  5.511613e+00   \n",
       "min   -8.676929e+01 -1.241699e+02 -4.357811e+01  0.000000e+00 -1.305874e+02   \n",
       "25%    5.333240e-02 -1.443266e+00 -1.936956e+01  1.826602e+00 -4.755968e-01   \n",
       "50%    5.347195e-01 -6.565638e-01 -1.075332e+01  4.145495e+00  2.601054e-01   \n",
       "75%    1.353378e+00 -2.225964e-01 -3.809207e+00  7.736388e+00  1.247540e+00   \n",
       "max    1.937742e+02  1.046825e+02  6.099917e-07  1.939003e+02  1.768665e+02   \n",
       "\n",
       "           strength        frazil        congel          Tair         trsig  \\\n",
       "count  2.142861e+06  2.142861e+06  2.142861e+06  2.142861e+06  2.142861e+06   \n",
       "mean   1.076691e+04  2.352717e-01  4.932120e-01 -1.093232e+01 -8.488240e+03   \n",
       "std    9.596997e+03  4.552266e-01  6.343287e-01  8.595249e+00  1.015034e+04   \n",
       "min    2.019805e-05  0.000000e+00  0.000000e+00 -3.681859e+01 -4.393299e+05   \n",
       "25%    3.588815e+03  1.932373e-02  1.497240e-02 -1.757928e+01 -1.192001e+04   \n",
       "50%    9.620862e+03  1.003949e-01  2.935034e-01 -9.761654e+00 -5.596223e+03   \n",
       "75%    1.556972e+04  2.692455e-01  7.037333e-01 -2.772018e+00 -1.606109e+03   \n",
       "max    3.218868e+05  1.397282e+01  7.270258e+00  1.498381e+00  0.000000e+00   \n",
       "\n",
       "       ...       strintx       strinty       strcorx       strcory  \\\n",
       "count  ...  2.142861e+06  2.142861e+06  2.142861e+06  2.142861e+06   \n",
       "mean   ...  2.743997e-03  2.104725e-03 -2.631413e-03  9.051661e-05   \n",
       "std    ...  2.530699e-02  2.216662e-02  1.404996e-02  1.792450e-02   \n",
       "min    ... -4.714528e-01 -8.035051e-01 -1.460927e-01 -2.079815e-01   \n",
       "25%    ... -5.014659e-03 -3.748889e-03 -8.397230e-03 -6.340887e-03   \n",
       "50%    ...  1.316632e-06  1.612066e-03 -1.699846e-03  8.445031e-04   \n",
       "75%    ...  7.168001e-03  1.000495e-02  2.763798e-03  8.587679e-03   \n",
       "max    ...  1.001443e+00  8.877492e-01  1.287962e-01  1.221347e-01   \n",
       "\n",
       "        wave_sig_ht   peak_period           sst        frzmlt     longitude  \\\n",
       "count  2.142861e+06  2.142861e+06  2.142861e+06  2.142861e+06  2.142861e+06   \n",
       "mean   9.599044e-02  1.206189e+01 -1.890005e+00 -2.592636e+01  2.068146e+02   \n",
       "std    3.869908e-01  6.324199e+00  2.250972e-01  1.169549e+02  9.924809e+01   \n",
       "min    0.000000e+00  0.000000e+00 -1.968267e+00 -1.000000e+03  5.000000e-01   \n",
       "25%    3.735831e-08  1.085520e+01 -1.926586e+00 -5.877564e+00  1.555000e+02   \n",
       "50%    9.152009e-05  1.478338e+01 -1.911775e+00  3.574283e-01  2.105000e+02   \n",
       "75%    1.047749e-02  1.479290e+01 -1.897585e+00  2.217344e+00  3.035000e+02   \n",
       "max    9.245866e+00  9.999902e+02  4.755164e+00  1.381290e+02  3.595000e+02   \n",
       "\n",
       "           latitude  \n",
       "count  2.142861e+06  \n",
       "mean  -6.849773e+01  \n",
       "std    5.010849e+00  \n",
       "min   -7.762990e+01  \n",
       "25%   -7.243740e+01  \n",
       "50%   -6.870660e+01  \n",
       "75%   -6.503162e+01  \n",
       "max   -5.303830e+01  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d67e5900-3e7f-4a2d-af67-61973a58ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "/g/data/ia40/cice-dirs/runs/waves-10/history/iceh.2018-01-01.nc\n",
      "Progress: [####################] 100% \n",
      "(6814861, 1)\n"
     ]
    }
   ],
   "source": [
    "# DAFSD terms\n",
    "\n",
    "variable_list = ['dafsd_latm','dafsd_latg','dafsd_weld','dafsd_newi','dafsd_wave']\n",
    "num_variables = np.size(variable_list)\n",
    "\n",
    "savepath = '/home/566/nd0349/notebooks/'\n",
    "mypath = '/g/data/ia40/cice-dirs/runs/waves-10/history/'\n",
    "year = 2018\n",
    "\n",
    "NFSD = np.asarray([2.6884, 9.7984, 21.6721, 40.7349, 70.1407, 113.6938, 175.5771, 259.8365, 369.6202, 506.2401, 668.2091, 850.4769])\n",
    "floe_binwidth = np.asarray([5.2438, 8.9763, 14.7711, 23.3545, 35.4569, 51.6493, 72.1173, 96.4015, 123.1658, 150.0741, 173.8638, 190.6719])\n",
    "\n",
    "\n",
    "os.chdir(mypath)\n",
    "file_dates = []\n",
    "print(year)\n",
    "filename =  mypath + 'iceh.' + str(year) + '-01-01.nc'\n",
    "\n",
    "onlyfiles = glob.glob(\"{path}/iceh.*{year}*\".format(path=mypath, year=year))\n",
    "onlyfiles.sort()\n",
    "print(filename)\n",
    "ds = xr.open_dataset(filename)\n",
    "LN = ds.TLON.values\n",
    "LT = ds.TLAT.values\n",
    "# Get the total number of grid points\n",
    "size = 1\n",
    "for dim in np.shape(LN): size *= dim\n",
    "aice_data = ds['aice'][0,:,:]\n",
    "mask1 = np.ma.masked_where(LT > 0.0, aice_data)\n",
    "mask2 = np.ma.masked_where(aice_data < 0.15, aice_data)\n",
    "master_mask = mask1.mask | mask2.mask\n",
    "mask = master_mask\n",
    "\n",
    "X_out =  np.ma.masked_array(np.empty((size,1)), mask=mask)\n",
    "\n",
    "# Loop over the files in that year\n",
    "for filecount, file in enumerate(onlyfiles):\n",
    "    progressBar = \"\\rProgress: \" + ProgressBar(len(onlyfiles), filecount+1, 20, '#', '.')\n",
    "    ShowBar(progressBar)\n",
    "\n",
    "    # Open the file\n",
    "    filename = file\n",
    "    file_dates.append(np.datetime64(file[-13:-3]))\n",
    "    ds = xr.open_dataset(filename)\n",
    "\n",
    "    # Get and apply masks to remove the ocean\n",
    "    aice_data = ds['aice'][0,:,:]\n",
    "    mask1 = np.ma.masked_where(LT > 0.0, aice_data)\n",
    "    mask2 = np.ma.masked_where(aice_data < 0.15, aice_data)\n",
    "    master_mask = mask1.mask | mask2.mask\n",
    "    mask = master_mask\n",
    "\n",
    "    # Get all the variables\n",
    "    for counter, exp in enumerate(variable_list):\n",
    "        data3d = ds[exp][0,:,:,:]\n",
    "        data = np.zeros(LN.shape)\n",
    "        for nf in range(0,len(floe_binwidth)):\n",
    "            data += data3d[nf,:,:].values*floe_binwidth[nf]*NFSD[nf]\n",
    "            \n",
    "        data_masked = np.ma.masked_where(mask, data)\n",
    "        data_masked_vec = data_masked.compressed()\n",
    "        row_length, = data_masked_vec.shape\n",
    "\n",
    "        if counter == 0: \n",
    "            # First file, then initialise X_temp\n",
    "            X_single_file = data_masked_vec.reshape(row_length,1)\n",
    "        else:\n",
    "            # Else just concatenate the new data on\n",
    "            X_single_file = np.concatenate([X_single_file, data_masked_vec.reshape(row_length,1)],axis=1)\n",
    "\n",
    "    # Add on the corresponding coordinates\n",
    "    LN_masked = np.ma.masked_where(mask, LN)\n",
    "    LN_vec = LN_masked.compressed()\n",
    "    LT_masked = np.ma.masked_where(mask, LT)\n",
    "    LT_vec = LT_masked.compressed()\n",
    "    X_single_file = np.concatenate([X_single_file, LN_vec.reshape(row_length,1), LT_vec.reshape(row_length,1)],axis=1)\n",
    "\n",
    "    if filecount == 0: \n",
    "        # Day 1, then initialise the year file\n",
    "        X_year = X_single_file\n",
    "        datetime_vec =  np.tile(np.datetime64(file[-13:-3]),(row_length,1))\n",
    "    else:\n",
    "        X_year = np.concatenate([X_year, X_single_file],axis=0)\n",
    "        datetime_vec = np.concatenate([datetime_vec, np.tile(np.datetime64(file[-13:-3]),(row_length,1))],axis=0)\n",
    "# Save as dataframe\n",
    "df_raw = pd.DataFrame(X_year, columns = variable_list+['longitude','latitude'])#,'date'])\n",
    "df_raw['date'] = datetime_vec\n",
    "print(datetime_vec.shape)\n",
    "df_raw = df_raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18a78a12-8da5-4075-a817-60dd4d85e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/g/data/ia40/sea-ice-classification/dataframes/'\n",
    "savefilename = 'analysis_fsd_raw_'+str(year)+'.csv'\n",
    "df_raw.to_csv(savepath+savefilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "065ae730-a3a9-430d-9b45-9456074324cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/data/ia40/cice-dirs/runs/waves-10/history/iceh.2018-01-01.nc\n",
      "5.2438\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(300, 360)\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (300,360) (12,300) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(data3d[nf,:,:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mdot(data3d[:,:,nf]\u001b[38;5;241m.\u001b[39mvalues, floe_binwidth[nf]))\n\u001b[0;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata3d\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnf\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloe_binwidth\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (300,360) (12,300) "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.zeros(LN.shape)\n",
    "filename =  mypath + 'iceh.' + str(year) + '-01-01.nc'\n",
    "print(filename)\n",
    "#xr.open_dataset(filename)\n",
    "#data3d[:,:,nf].values*floe_binwidth[nf]\n",
    "\n",
    "#data += data3d[:,:,nf]*floe_binwidth[nf]\n",
    "print(floe_binwidth[nf])\n",
    "print(data)\n",
    "print(data3d[nf,:,:].shape)\n",
    "print(np.dot(data3d[:,:,nf].values, floe_binwidth[nf]))\n",
    "data = data + np.dot(data3d[nf,:,:].values, floe_binwidth[nf])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
